{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a8f832",
   "metadata": {},
   "source": [
    "# Visualization for Tree Crown Segmentation\n",
    "\n",
    "**Objective:** Load the `best_checkpoint` from the `TrueResSegformer_mit_b5_min_lr_0` run, re-create the exact validation split used during that training, run inference on a subset of these validation samples, and visualize the results with visualizations optimized for binary tree crown segmentation.\n",
    "\n",
    "## Key Enhancements:\n",
    "- **Natural colors**: Green for trees (instead of cyan)\n",
    "- **Enhanced error analysis**: Clear TP/FP/FN visualization\n",
    "- **Improved contrast**: Better visibility on various backgrounds\n",
    "- **Comprehensive analysis panels**: Multiple visualization modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a904d",
   "metadata": {},
   "source": [
    "## 1. Setup and Enhanced Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d540f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Define project root and add to sys.path for project-specific imports\n",
    "project_root_abs = \"/Users/fadil/Desktop/Git_Latex_Container\"\n",
    "if project_root_abs not in sys.path:\n",
    "    sys.path.append(project_root_abs)\n",
    "\n",
    "# Project-specific imports (ensure these files are in project_root_abs or adjust paths)\n",
    "from config import Config\n",
    "from dataset import load_and_shuffle_dataset # TCDDataset might be used by this function\n",
    "from checkpoint import load_model_for_evaluation\n",
    "\n",
    "# Enhanced visualization imports\n",
    "from visualization import (\n",
    "    visualize_segmentation,\n",
    "    tensor_to_image,\n",
    "    visualize_boundary_iou_components,\n",
    "    # NEW ENHANCED FUNCTIONS:\n",
    "    visualize_segmentation_enhanced,\n",
    "    visualize_error_decomposition,\n",
    "    plot_enhanced_segmentation_analysis,\n",
    "    create_confidence_visualization\n",
    ")\n",
    "from utils import get_logger, set_seed\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger()\n",
    "\n",
    "# Set seed (use the same seed as the TrueResSegformer_mit_b5_min_lr_0 run)\n",
    "SEED = 42 # From config.json of the run\n",
    "set_seed(SEED)\n",
    "logger.info(f\"Seed set to {SEED}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Figure saving configuration\n",
    "SAVE_FIGURES = True\n",
    "SAVE_DIR = os.path.join(project_root_abs, \"past_run_repository\", \"outputs_individual_figures\", \"validation_enhanced\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "SAVE_DPI = 300\n",
    "SAVE_FORMATS = [\"png\"]\n",
    "SAVE_TRANSPARENT = False\n",
    "logger.info(f\"Saving figures to: {SAVE_DIR} (formats: {SAVE_FORMATS}, dpi: {SAVE_DPI})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b75d3",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(project_root_abs, \"/Users/fadil/Desktop/Git_Latex_Container/past_run_repository/SETR_Ablation/setr_full_res/outputs_setr_full_res/config.json\")\n",
    "config_dict = None\n",
    "if not os.path.exists(config_path):\n",
    "    logger.error(f\"Config file not found at {config_path}. Please ensure the path is correct.\")\n",
    "else:\n",
    "    config_dict = Config.load(config_path) # Config.load returns a dict\n",
    "    logger.info(f\"Successfully loaded configuration from {config_path}\")\n",
    "    logger.info(f\"Run Model Name: {config_dict.get('model_name')}\")\n",
    "    logger.info(f\"Run Dataset Name: {config_dict.get('dataset_name')}\")\n",
    "    logger.info(f\"Run Seed: {config_dict.get('seed')}\")\n",
    "    logger.info(f\"Run Validation Split: {config_dict.get('validation_split')}\")\n",
    "    # Ensure the seed used for the notebook matches the run's seed\n",
    "    if config_dict.get('seed') != SEED:\n",
    "        logger.warning(f\"Mismatch between notebook seed ({SEED}) and run config seed ({config_dict.get('seed')}). Using notebook seed for general operations, but run's seed for dataset splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa0fc7",
   "metadata": {},
   "source": [
    "## 3. Load Model from Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6201e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_resolution = {\"height\": 1024, \"width\": 1024}\n",
    "\n",
    "model_checkpoint_path = os.path.join(project_root_abs, \"/Users/fadil/Desktop/Git_Latex_Container/past_run_repository/SETR_Ablation/setr_full_res/outputs_setr_full_res/best_checkpoint\")\n",
    "\n",
    "logger.info(f\"Attempting to load model from: {model_checkpoint_path}\")\n",
    "\n",
    "model, image_processor = None, None\n",
    "if not config_dict:\n",
    "    logger.error(\"Config dictionary not loaded. Cannot load model.\")\n",
    "elif not os.path.exists(os.path.join(model_checkpoint_path, 'pytorch_model.bin')):\n",
    "    logger.error(f\"Checkpoint 'pytorch_model.bin' not found in {model_checkpoint_path}. Please ensure the path is correct.\")\n",
    "else:\n",
    "    model, image_processor = load_model_for_evaluation(\n",
    "        model_path=model_checkpoint_path,\n",
    "        config=config_dict, # Pass the loaded run-specific config dictionary\n",
    "        device=device,\n",
    "        logger=logger\n",
    "    )\n",
    "    if model and image_processor:\n",
    "        model.eval()\n",
    "        logger.info(\"Model and image processor loaded successfully and model set to evaluation mode.\")\n",
    "    else:\n",
    "        logger.error(\"Failed to load model and image processor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b023f",
   "metadata": {},
   "source": [
    "## 4. Re-create the Validation Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64885526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset # For Hugging Face datasets.Dataset object\n",
    "\n",
    "validation_hf_dataset = None\n",
    "if config_dict:\n",
    "    run_seed = config_dict['seed'] # IMPORTANT: Use the seed from the specific run's config for splitting\n",
    "    logger.info(f\"Loading dataset: {config_dict['dataset_name']} with seed for shuffling: {run_seed}\")\n",
    "    full_dataset_dict = load_and_shuffle_dataset(\n",
    "        dataset_name=config_dict[\"dataset_name\"],\n",
    "        seed=run_seed # Use the seed from the loaded config for initial shuffle\n",
    "    )\n",
    "\n",
    "    original_train_dataset = full_dataset_dict[\"train\"]\n",
    "    logger.info(f\"Original 'train' split (before val split) has {len(original_train_dataset)} samples.\")\n",
    "\n",
    "    validation_split_ratio = config_dict[\"validation_split\"]\n",
    "    num_total_original_train_samples = len(original_train_dataset)\n",
    "    num_val_samples = int(validation_split_ratio * num_total_original_train_samples)\n",
    "\n",
    "    # Ensure consistent split by using the same seed as the original run for randperm\n",
    "    generator = torch.Generator().manual_seed(run_seed)\n",
    "    indices = torch.randperm(num_total_original_train_samples, generator=generator).tolist()\n",
    "\n",
    "    val_indices = indices[:num_val_samples]\n",
    "\n",
    "    # Create a Hugging Face Dataset object for the validation split\n",
    "    validation_hf_dataset = original_train_dataset.select(val_indices)\n",
    "\n",
    "    logger.info(f\"Re-created validation split with {len(validation_hf_dataset)} samples using seed {run_seed} for permutation.\")\n",
    "else:\n",
    "    logger.error(\"Config dictionary not loaded. Cannot re-create dataset split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c3000",
   "metadata": {},
   "source": [
    "## 5. Enhanced Visualization for Tree Crown Segmentation\n",
    "\n",
    "This section provides multiple visualization modes optimized for binary tree crown segmentation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model and image_processor and validation_hf_dataset and config_dict:\n",
    "    num_to_visualize = min(10, len(validation_hf_dataset)) # Visualize up to 5 samples\n",
    "    logger.info(f\"Will visualize {num_to_visualize} samples with enhanced visualizations.\")\n",
    "\n",
    "    # Get id2label from config, provide a fallback if not present\n",
    "    id2label = config_dict.get('id2label', {0: 'background', 1: 'tree_crown'}) \n",
    "    # Ensure keys are integers for visualization function\n",
    "    if id2label and isinstance(next(iter(id2label.keys())), str):\n",
    "        id2label = {int(k): v for k, v in id2label.items()}\n",
    "\n",
    "    for i in range(num_to_visualize):\n",
    "        logger.info(f\"Processing sample {i+1}/{num_to_visualize}\")\n",
    "        raw_sample = validation_hf_dataset[i]\n",
    "\n",
    "        pil_image = raw_sample['image'].convert(\"RGB\") # Ensure image is RGB\n",
    "        gt_mask_pil = raw_sample.get('annotation', raw_sample.get('label')) \n",
    "        if gt_mask_pil is None:\n",
    "            logger.error(f\"Ground truth mask not found for sample {i}. Skipping.\")\n",
    "            continue\n",
    "        gt_mask_np = np.array(gt_mask_pil)\n",
    "\n",
    "        original_gt_shape = gt_mask_np.shape \n",
    "        logger.info(f\"Initial gt_mask_np shape: {original_gt_shape}, unique values: {np.unique(gt_mask_np)[:20]}\")\n",
    "\n",
    "        # Process ground truth mask\n",
    "        if gt_mask_np.ndim == 3 and gt_mask_np.shape[-1] == 3:\n",
    "            logger.info(f\"Ground truth mask is 3-channel. Converting to binary mask.\")\n",
    "            gt_mask_np = (gt_mask_np.sum(axis=-1) > 0).astype(np.uint8)\n",
    "        elif gt_mask_np.ndim == 3 and gt_mask_np.shape[-1] == 1:\n",
    "            logger.info(f\"Ground truth mask is single-channel 3D. Squeezing to 2D.\")\n",
    "            gt_mask_np = np.squeeze(gt_mask_np, axis=-1)\n",
    "        elif gt_mask_np.ndim == 2:\n",
    "            logger.info(f\"Ground truth mask is already 2D. Shape: {gt_mask_np.shape}\")\n",
    "\n",
    "        # Remap 255 values to 1 for binary case\n",
    "        if 1 in id2label and np.any(gt_mask_np == 255):\n",
    "            logger.info(f\"Remapping gt_mask_np values of 255 to 1 for tree crown class.\")\n",
    "            gt_mask_np[gt_mask_np == 255] = 1\n",
    "        \n",
    "        # Perform Prediction\n",
    "        training_image_size_config = config_dict.get(\"image_size\", 1024)\n",
    "        if isinstance(training_image_size_config, int):\n",
    "            target_size = {\"height\": training_image_size_config, \"width\": training_image_size_config}\n",
    "        else:\n",
    "            target_size = {\"height\": 1024, \"width\": 1024}\n",
    "\n",
    "        logger.info(f\"Resizing input image from {pil_image.size} to: {target_size} for model inference.\")\n",
    "        inputs = image_processor(\n",
    "            images=pil_image, \n",
    "            return_tensors=\"pt\",\n",
    "            do_resize=True,\n",
    "            size=target_size,\n",
    "            resample=Image.Resampling.BILINEAR,\n",
    "            do_rescale=True,\n",
    "            do_normalize=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        original_h, original_w = pil_image.height, pil_image.width\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=(original_h, original_w),\n",
    "            mode=config_dict.get(\"interpolation_mode\", \"bilinear\"),\n",
    "            align_corners=config_dict.get(\"interpolation_align_corners\", False)\n",
    "        )\n",
    "        \n",
    "        predicted_mask_np = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        logger.info(f\"Prediction complete. Unique values: {np.unique(predicted_mask_np)}\")\n",
    "\n",
    "        # Convert PIL image to numpy for visualization\n",
    "        image_np = np.array(pil_image)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SAMPLE {i+1} - TREE CROWN SEGMENTATION ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # === 1. COMPREHENSIVE ENHANCED ANALYSIS PANEL ===\n",
    "        logger.info(f\"Creating comprehensive analysis panel for sample {i+1}\")\n",
    "        plot_enhanced_segmentation_analysis(\n",
    "            image=image_np,\n",
    "            prediction=predicted_mask_np,\n",
    "            ground_truth=gt_mask_np,\n",
    "            sample_id=f\"Sample {i+1}\",\n",
    "            figsize=(25, 12),\n",
    "            id2label=id2label,\n",
    "            include_boundary_analysis=True,\n",
    "            include_error_decomposition=True\n",
    "        )\n",
    "        # Skip saving comprehensive (multi-panel) figure; focus on individual PNGs only\n",
    "        fig_comp = plt.gcf()\n",
    "        plt.show()\n",
    "\n",
    "        # === 2. VISUALIZATION (INDIVIDUAL FIGURES) ===\n",
    "        print(\"\\nVisualization (Individual Figures):\")\n",
    "        sample_prefix = f\"sample_{i+1:03d}\"\n",
    "\n",
    "        # Original image (individual)\n",
    "        fig_o, ax_o = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_o.imshow(image_np)\n",
    "        ax_o.set_title(\"Original Image\", fontsize=14)\n",
    "        ax_o.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_original.{fmt}\")\n",
    "                fig_o.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Enhanced GT (individual)\n",
    "        gt_enhanced = visualize_segmentation_enhanced(\n",
    "            image_np.copy(), gt_mask_np, id2label=id2label, \n",
    "            alpha=0.6, use_natural_colors=True, high_contrast=True\n",
    "        )\n",
    "        fig_gt, ax_gt = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_gt.imshow(gt_enhanced)\n",
    "        ax_gt.set_title(\"Ground Truth (Overlay)\", fontsize=14)\n",
    "        ax_gt.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_gt_overlay.{fmt}\")\n",
    "                fig_gt.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Enhanced Prediction (individual)\n",
    "        pred_enhanced = visualize_segmentation_enhanced(\n",
    "            image_np.copy(), predicted_mask_np, id2label=id2label,\n",
    "            alpha=0.6, use_natural_colors=True, high_contrast=True\n",
    "        )\n",
    "        fig_pr, ax_pr = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_pr.imshow(pred_enhanced)\n",
    "        ax_pr.set_title(\"Prediction (Overlay)\", fontsize=14)\n",
    "        ax_pr.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_prediction_overlay.{fmt}\")\n",
    "                fig_pr.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # === 3. DETAILED ERROR ANALYSIS (INDIVIDUAL FIGURES) ===\n",
    "        print(\"\\nDetailed Error Analysis (Individual):\")\n",
    "        # Error decomposition (individual)\n",
    "        error_vis = visualize_error_decomposition(\n",
    "            image_np.copy(), predicted_mask_np, gt_mask_np, \n",
    "            alpha=0.8, use_natural_colors=True\n",
    "        )\n",
    "        fig_err, ax_err = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_err.imshow(error_vis)\n",
    "        ax_err.set_title(\"Error Analysis\\nGreen: Correct Trees, Yellow: False Alarms, Red: Missed Trees\", fontsize=14)\n",
    "        ax_err.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_error_decomposition.{fmt}\")\n",
    "                fig_err.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Boundary analysis (individual)\n",
    "        boundary_vis = visualize_boundary_iou_components(\n",
    "            image_np.copy(), gt_mask_np, predicted_mask_np,\n",
    "            dilation_pixels=5, alpha=0.8\n",
    "        )\n",
    "        fig_ba, ax_ba = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_ba.imshow(boundary_vis)\n",
    "        ax_ba.set_title(\"Boundary Analysis\\nGreen: Correct Boundaries, Yellow: False Boundaries, Red: Missed Boundaries\", fontsize=14)\n",
    "        ax_ba.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_boundary_analysis.{fmt}\")\n",
    "                fig_ba.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # === 4. GROUND TRUTH BOUNDARY (INDIVIDUAL FIGURE) ===\n",
    "        logger.info(f\"Rendering ground truth boundary overlay for sample {i+1}\")\n",
    "        # Ensure binary mask (0/1)\n",
    "        gt_binary = (gt_mask_np == 1).astype(np.uint8)\n",
    "        fig_gb, ax_gb = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax_gb.imshow(image_np)\n",
    "        ax_gb.contour(gt_binary, levels=[0.5], colors='lime', linewidths=2)\n",
    "        ax_gb.set_title(f\"Sample {i+1} - Ground Truth Boundary (Overlay)\", fontsize=14)\n",
    "        ax_gb.axis('off')\n",
    "        if SAVE_FIGURES:\n",
    "            for fmt in SAVE_FORMATS:\n",
    "                out_path = os.path.join(SAVE_DIR, f\"{sample_prefix}_gt_boundary_contour.{fmt}\")\n",
    "                fig_gb.savefig(out_path, dpi=SAVE_DPI, bbox_inches='tight', pad_inches=0.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Cannot proceed with visualization. Ensure Model, Image Processor, Validation Dataset, and Config are available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## Summary of Visualizations\n",
    "\n",
    "This notebook provides several key improvements for binary tree crown segmentation analysis:\n",
    "\n",
    "### 🎨 **Visual Enhancements:**\n",
    "- **Natural Colors**: Green for trees instead of generic cyan/blue\n",
    "- **High Contrast**: Better visibility against various backgrounds\n",
    "- **Optimized Alpha Blending**: Different transparency levels for different information types\n",
    "\n",
    "### 📊 **Analysis Capabilities:**\n",
    "- **Comprehensive Analysis Panel**: 5-panel view with original, GT, prediction, error decomposition, and boundary analysis\n",
    "- **Error Decomposition**: Clear visualization of TP (green), FP (red), FN (yellow)\n",
    "- **Boundary Analysis**: Focus on edge/boundary errors critical for tree crown delineation\n",
    "- **Enhanced Visualization**: Clean single-row visualization comparison\n",
    "\n",
    "### 🎯 **Tree Crown Specific Features:**\n",
    "- Optimized for binary segmentation (background vs tree crown)\n",
    "- Natural forest/vegetation color scheme\n",
    "- Enhanced boundary focus (critical for crown delineation)\n",
    "- Intuitive error color coding for forestry applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
