{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BARE Pipeline Implementation\n",
    "\n",
    "This notebook implements a complete end-to-end pipeline for training, evaluating, and deploying the BARE model for tree crown delineation using the refactored codebase.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Setup & Configuration\n",
    "2. Dataset Loading & Exploration\n",
    "3. Model Creation\n",
    "4. Training Pipeline\n",
    "5. Evaluation & Metrics\n",
    "6. Prediction on New Images\n",
    "7. Advanced Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Ignore specific warnings if needed (e.g., from PIL)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='PIL')\n",
    "\n",
    "# Import necessary modules from the codebase\n",
    "from config import Config, adjust_config_for_architecture\n",
    "from dataset import load_and_shuffle_dataset, create_dataloaders\n",
    "from model import create_model\n",
    "from pipeline import run_training_pipeline, evaluate_model, run_prediction_pipeline\n",
    "from visualization import (\n",
    "    plot_worst_predictions, \n",
    "    plot_confusion_matrix, \n",
    "    plot_error_analysis_map, \n",
    "    visualize_segmentation\n",
    ")\n",
    "from metrics import calculate_metrics # For potential re-evaluation\n",
    "from utils import get_logger, set_seed\n",
    "from checkpoint import load_model_for_evaluation\n",
    "\n",
    "# Ensure plots are displayed inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "logger = get_logger()\n",
    "\n",
    "# Instantiate the configuration using defaults\n",
    "config = Config()\n",
    "\n",
    "# Adjust configuration based on the architecture specified in config.py\n",
    "config = adjust_config_for_architecture(config)\n",
    "\n",
    "logger.info(f\"Configuration loaded for '{config['architecture']}' architecture.\")\n",
    "logger.info(f\"Output directory: {config['output_dir']}\")\n",
    "logger.info(f\"Using seed: {config['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Available CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration and Setup Logging\n",
    "\n",
    "We'll instantiate the `Config` object using the defaults defined in `config.py` (which now match the reference model) and set up logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save config to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert config to a dictionary\n",
    "if hasattr(config, 'to_dict'):\n",
    "    config_dict = config.to_dict()\n",
    "elif isinstance(config, dict):\n",
    "    config_dict = config\n",
    "else:\n",
    "    config_dict = dict(config)\n",
    "\n",
    "# Define the path\n",
    "config_save_path = os.path.join(config['output_dir'], 'config.json')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "# Save the configuration as JSON\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "\n",
    "logger.info(f\"Configuration saved to {config_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading & Exploration\n",
    "\n",
    "Let's load the Tree Crown Delineation (TCD) dataset and explore its characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset using `load_and_shuffle_dataset` for consistency and then create the dataloaders using `create_dataloaders`. We'll also display a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle the dataset\n",
    "logger.info(f\"Loading dataset: {config['dataset_name']}\")\n",
    "dataset_dict = load_and_shuffle_dataset(\n",
    "    dataset_name=config['dataset_name'], \n",
    "    seed=config['seed']\n",
    ")\n",
    "\n",
    "# --- CORRECTED LOGIC FOR SETR ---\n",
    "# For SETR, we pass target_size=None to create_dataloaders to ensure no resizing is done at the dataset level.\n",
    "# The native 1024x1024 processing is handled inside the SETRWrapper.\n",
    "target_size = None\n",
    "if config['architecture'] == 'setr':\n",
    "    logger.info(f\"Architecture is SETR. No resizing at data loading stage to allow native {config['setr_input_size']}px processing.\")\n",
    "else:\n",
    "    logger.info(f\"Architecture is '{config['architecture']}'. No final resizing will be applied.\")\n",
    "# --- END CORRECTION ---\n",
    "\n",
    "# Create dataloaders\n",
    "logger.info(\"Creating dataloaders...\")\n",
    "train_dataloader, eval_dataloader, id2label, label2id = create_dataloaders(\n",
    "    dataset_dict=dataset_dict,\n",
    "    image_processor=None,  # Will be created internally with do_resize=False\n",
    "    config=config, \n",
    "    train_batch_size=config['train_batch_size'],\n",
    "    eval_batch_size=config['eval_batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    validation_split=config['validation_split'],\n",
    "    seed=config['seed'],\n",
    "    target_size=target_size  # This will be None for both architectures now\n",
    ")\n",
    "\n",
    "# Update config with actual id2label mapping (important for model creation)\n",
    "config['id2label'] = id2label\n",
    "config['label2id'] = label2id\n",
    "\n",
    "logger.info(f\"Dataloaders created. Train batches: {len(train_dataloader)}, Eval batches: {len(eval_dataloader)}\")\n",
    "logger.info(f\"Class mapping: {id2label}\")\n",
    "\n",
    "# Display a sample from the training dataloader\n",
    "try:\n",
    "    batch = next(iter(train_dataloader))\n",
    "    img_tensor = batch['pixel_values'][0]\n",
    "    lbl_tensor = batch['labels'][0]\n",
    "    \n",
    "    # Convert tensor back to displayable image (denormalize if needed - processor handles this)\n",
    "    # Assuming processor output is normalized [-1, 1] or [0, 1]\n",
    "    img_display = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    # Denormalize based on typical ImageNet stats used by SegFormer processor\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_display = std * img_display + mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    lbl_display = lbl_tensor.cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(img_display)\n",
    "    ax[0].set_title('Sample Image (Augmented)')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(lbl_display, cmap='gray') # Assuming binary mask\n",
    "    ax[1].set_title('Sample Mask (Augmented)')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not display sample batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Creation\n",
    "\n",
    "Now we'll create the SegFormer model using the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of training steps for scheduler (if needed, though ReduceLROnPlateau doesn't use it initially)\n",
    "num_training_steps = len(train_dataloader) * config['num_epochs'] // config['gradient_accumulation_steps']\n",
    "\n",
    "# Create model, optimizer, and scheduler\n",
    "logger.info(\"Creating model...\")\n",
    "model, optimizer, scheduler = create_model(\n",
    "    config=config, \n",
    "    num_training_steps=num_training_steps,\n",
    "    logger=logger # Pass the logger\n",
    ")\n",
    "\n",
    "# Print model summary (optional)\n",
    "from torchinfo import summary\n",
    "\n",
    "# --- ARCHITECTURE-AWARE INPUT SIZE FOR SUMMARY ---\n",
    "if config['architecture'] == 'setr':\n",
    "    input_size_h = config['setr_input_size']\n",
    "    input_size_w = config['setr_input_size']\n",
    "    logger.info(f\"Using SETR input size for model summary: {input_size_h}x{input_size_w}\")\n",
    "else:\n",
    "    input_size_h = config['augmentation']['random_crop_size']\n",
    "    input_size_w = config['augmentation']['random_crop_size']\n",
    "    logger.info(f\"Using augmentation crop size for model summary: {input_size_h}x{input_size_w}\")\n",
    "\n",
    "input_size = (config['train_batch_size'], 3, input_size_h, input_size_w) \n",
    "try:\n",
    "    summary(model, input_size=input_size)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not print model summary: {e}\")\n",
    "\n",
    "logger.info(\"Model, optimizer, and scheduler created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline\n",
    "\n",
    "Run the training using the `run_training_pipeline` function, which handles the complete loop, evaluation during training, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training pipeline\n",
    "logger.info(\"Starting training pipeline...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "training_results = run_training_pipeline(\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    is_notebook=True # Let the pipeline know it's running in a notebook for tqdm compatibility\n",
    ")\n",
    "\n",
    "# Calculate training duration\n",
    "training_duration = time.time() - training_start_time\n",
    "print(f\"\\nTraining finished in {training_duration / 60:.2f} minutes.\")\n",
    "\n",
    "# The trained model is available in training_results['model']\n",
    "# Final metrics are in training_results['metrics']\n",
    "# Location of saved model artifacts is in training_results['model_dir']\n",
    "logger.info(f\"Training finished. Final model saved to: {training_results['model_dir']}\")\n",
    "logger.info(f\"Final evaluation metrics from training: {training_results['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computional Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational Cost Reporting\n",
    "logger.info(\"\\n--- Computational Cost Analysis ---\")\n",
    "\n",
    "# 1. Model Parameters\n",
    "if 'model' in training_results and training_results['model'] is not None:\n",
    "    model = training_results['model']\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Total Model Parameters: {total_params:,}\")\n",
    "    logger.info(f\"Trainable Model Parameters: {trainable_params:,}\")\n",
    "    print(f\"Total Model Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Model Parameters: {trainable_params:,}\")\n",
    "else:\n",
    "    logger.warning(\"Model not found in training_results. Skipping parameter count.\")\n",
    "    print(\"Model not found in training_results. Skipping parameter count.\")\n",
    "\n",
    "# 2. FLOPs Estimation (using thop library)\n",
    "try:\n",
    "    from thop import profile\n",
    "    if 'model' in training_results and training_results['model'] is not None:\n",
    "        model_device = next(model.parameters()).device # Get device from model\n",
    "        \n",
    "        # --- ARCHITECTURE-AWARE INPUT SIZE FOR FLOPS ---\n",
    "        if config['architecture'] == 'setr':\n",
    "            H = W = config['setr_input_size']\n",
    "            logger.info(f\"Using SETR input size for FLOPs calculation: {H}x{W}\")\n",
    "        else:\n",
    "            H = W = config['augmentation']['random_crop_size']\n",
    "            logger.info(f\"Using augmentation crop size for FLOPs calculation: {H}x{W}\")\n",
    "        \n",
    "        input_channels = 3\n",
    "        \n",
    "        # Create a dummy input tensor on the same device as the model\n",
    "        dummy_input = torch.randn(1, input_channels, H, W).to(model_device)\n",
    "        \n",
    "        macs, params_thop = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "        gflops = macs * 2 / 1e9  # MACs to FLOPs (multiply by 2) and then to GFLOPs\n",
    "        logger.info(f\"Estimated GFLOPs: {gflops:.2f} GFLOPs\")\n",
    "        print(f\"Estimated GFLOPs: {gflops:.2f} GFLOPs (using input size {input_channels}x{H}x{W})\")\n",
    "    else:\n",
    "        logger.warning(\"Model not found for FLOPs calculation.\")\n",
    "        print(\"Model not found for FLOPs calculation.\")\n",
    "except ImportError:\n",
    "    logger.warning(\"thop library not found. Skipping FLOPs calculation. Install with 'pip install thop'\")\n",
    "    print(\"thop library not found. Skipping FLOPs calculation. Install with 'pip install thop'\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during FLOPs calculation: {e}\")\n",
    "    print(f\"Error during FLOPs calculation: {e}\")\n",
    "\n",
    "# 3. Inference Time\n",
    "logger.info(\"Attempting to measure inference time...\")\n",
    "if 'model' in training_results and training_results['model'] is not None:\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_dataloader = None # Initialize to None\n",
    "\n",
    "    if 'eval_dataloader' in locals() or 'eval_dataloader' in globals():\n",
    "        val_dataloader = eval_dataloader \n",
    "        logger.info(f\"Using existing 'eval_dataloader' for inference time measurement. Batches: {len(val_dataloader)}\")\n",
    "    else:\n",
    "        logger.warning(\"'eval_dataloader' not found. Skipping inference time measurement.\")\n",
    "\n",
    "    if val_dataloader:\n",
    "        total_inference_time = 0\n",
    "        num_samples = 0\n",
    "        num_batches = 0\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                if isinstance(batch, dict) and 'pixel_values' in batch:\n",
    "                    inputs = batch['pixel_values'].to(device)\n",
    "                elif isinstance(batch, (list, tuple)):\n",
    "                    inputs = batch[0].to(device)\n",
    "                else:\n",
    "                    inputs = batch.to(device)\n",
    "\n",
    "                start_time = time.time()\n",
    "                _ = model(inputs)\n",
    "                if device.type == 'cuda': torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_inference_time += (end_time - start_time)\n",
    "                num_samples += inputs.size(0)\n",
    "                num_batches += 1\n",
    "                if num_batches >= 20: # Limit for estimation\n",
    "                    logger.info(\"Inference timing based on the first 20 batches.\")\n",
    "                    break \n",
    "        \n",
    "        if num_batches > 0:\n",
    "            avg_time_per_batch = total_inference_time / num_batches\n",
    "            avg_time_per_sample = total_inference_time / num_samples\n",
    "            logger.info(f\"Average Inference Time per Batch: {avg_time_per_batch:.4f} seconds\")\n",
    "            logger.info(f\"Average Inference Time per Sample: {avg_time_per_sample:.4f} seconds\")\n",
    "            print(f\"Average Inference Time per Batch: {avg_time_per_batch:.4f} seconds\")\n",
    "            print(f\"Average Inference Time per Sample: {avg_time_per_sample:.4f} seconds\")\n",
    "        else:\n",
    "            logger.warning(\"No batches processed. Cannot calculate inference time.\")\n",
    "            print(\"WARNING: No batches processed. Cannot calculate inference time.\")\n",
    "else:\n",
    "    logger.warning(\"Model not found. Skipping inference time calculation.\")\n",
    "    print(\"Model not found. Skipping inference time calculation.\")\n",
    "\n",
    "# 4. GPU Memory Usage\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory_allocated = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    peak_memory_reserved = torch.cuda.max_memory_reserved() / (1024**2)\n",
    "    logger.info(f\"Peak GPU Memory Allocated: {peak_memory_allocated:.2f} MB\")\n",
    "    logger.info(f\"Peak GPU Memory Reserved: {peak_memory_reserved:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Allocated: {peak_memory_allocated:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Reserved: {peak_memory_reserved:.2f} MB\")\n",
    "else:\n",
    "    logger.info(\"CUDA not available. Skipping GPU memory usage reporting.\")\n",
    "\n",
    "logger.info(\"--- End of Computational Cost Analysis ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Metrics\n",
    "\n",
    "The `run_training_pipeline` already performs evaluation. Here, we can explicitly re-evaluate the final saved model and visualize metrics like the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to evaluate either a specific checkpoint or run the best model\n",
    "\n",
    "# Either use the best model path returned by training\n",
    "best_model_path = training_results.get('best_model_dir') or training_results['model_dir']\n",
    "logger.info(f\"Evaluating best model from training: {best_model_path}\")\n",
    "\n",
    "# Or specify an explicit path to a previous model checkpoint\n",
    "# best_model_path = \"past_run_repository/outputs_pspnet/best_checkpoint\"\n",
    "\n",
    "# Load model with the new function - handles both standard and BARESegformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_eval, image_processor_eval = load_model_for_evaluation(\n",
    "    model_path=best_model_path,\n",
    "    config=config,  # Pass your current config\n",
    "    device=device,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Create dataloader for evaluation\n",
    "_, eval_dataloader_reeval, _, _ = create_dataloaders(\n",
    "    dataset_dict=dataset_dict,  # Assuming this is available from earlier\n",
    "    image_processor=image_processor_eval,\n",
    "    config=config,\n",
    "    train_batch_size=config['train_batch_size'],\n",
    "    eval_batch_size=config['eval_batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    validation_split=config['validation_split'],\n",
    "    seed=config['seed']\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "logger.info(\"Running evaluation on the loaded model...\")\n",
    "metrics = evaluate_model(\n",
    "    model=model_eval,\n",
    "    eval_dataloader=eval_dataloader_reeval,\n",
    "    device=device,\n",
    "    output_dir=config['output_dir'],\n",
    "    id2label=config['id2label'],\n",
    "    visualize_worst=True,\n",
    "    num_worst_samples=5,\n",
    "    visualize_confidence_comparison=False,\n",
    "    analyze_errors=True,\n",
    "    logger=logger,\n",
    "    is_notebook=True\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "logger.info(f\"Evaluation metrics: {metrics}\")\n",
    "\n",
    "# If you want to visualize the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "cm_norm_path = os.path.join(config['output_dir'], \"confusion_matrix_normalized.png\")\n",
    "if os.path.exists(cm_norm_path):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(Image.open(cm_norm_path))\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction on New Images\n",
    "\n",
    "Use the `run_prediction_pipeline` function to predict segmentation masks on new images. We'll need some sample images for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction Setup ---\n",
    "# Set path to the actual image file\n",
    "image_path = \"test_image.tif\"\n",
    "\n",
    "# Check if image exists\n",
    "if not os.path.exists(image_path):\n",
    "    logger.error(f\"Image not found at {image_path}. Please verify the path.\")\n",
    "else:\n",
    "    logger.info(f\"Found image at {image_path}. Proceeding with prediction...\")\n",
    "    \n",
    "    # Define prediction output directory\n",
    "    prediction_output_dir = os.path.join(config['output_dir'], 'predictions')\n",
    "    \n",
    "    # Run prediction pipeline\n",
    "    try:\n",
    "        logger.info(f\"Running prediction using model from {training_results['model_dir']}...\")\n",
    "        \n",
    "        prediction_results = run_prediction_pipeline(\n",
    "            config=config,\n",
    "            image_paths=image_path,  # Just passing the single image path\n",
    "            model_path=training_results['model_dir'],  # Use the trained model directory\n",
    "            output_dir=prediction_output_dir,\n",
    "            visualize=True,\n",
    "            show_confidence=True,  # Show confidence maps\n",
    "            logger=logger,\n",
    "            is_notebook=True\n",
    "        )\n",
    "        \n",
    "        # Display the prediction visualization\n",
    "        if prediction_results['visualizations']:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(prediction_results['visualizations'][0])\n",
    "            plt.title(f\"Prediction: {os.path.basename(image_path)}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        # Display the confidence map if generated\n",
    "        if prediction_results.get('confidence_maps') and prediction_results['confidence_maps']:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(prediction_results['confidence_maps'][0])\n",
    "            plt.title(f\"Confidence Map: {os.path.basename(image_path)}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        logger.info(f\"Prediction finished. Visualizations saved to: {prediction_output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction pipeline failed: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Visualization & Analysis\n",
    "\n",
    "Explore advanced visualizations like error analysis maps generated during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved error analysis map for one of the worst samples (if generated)\n",
    "error_analysis_dir = os.path.join(config['output_dir'], 'error_analysis')\n",
    "if os.path.exists(error_analysis_dir):\n",
    "    # Use glob to find any error map files rather than hardcoding a specific name\n",
    "    error_maps = glob.glob(os.path.join(error_analysis_dir, 'error_map_*.png'))\n",
    "    \n",
    "    if error_maps:\n",
    "        error_map_to_show = error_maps[0]  # Show the first one found\n",
    "        logger.info(f\"Displaying error analysis map: {error_map_to_show}\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(Image.open(error_map_to_show))\n",
    "        plt.title(f\"Error Analysis Map: {os.path.basename(error_map_to_show)}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.warning(f\"No error analysis maps found in {error_analysis_dir}.\")\n",
    "else:\n",
    "    logger.warning(f\"Error analysis directory not found at {error_analysis_dir}. Ensure 'analyze_errors=True' during evaluation.\")\n",
    "\n",
    "# Display saved worst prediction visualization (if generated)\n",
    "worst_pred_dir = os.path.join(config['output_dir'], 'worst_predictions')\n",
    "if os.path.exists(worst_pred_dir):\n",
    "    # Find all worst prediction files\n",
    "    worst_files = glob.glob(os.path.join(worst_pred_dir, 'worst_*.png'))\n",
    "    \n",
    "    if worst_files:\n",
    "        # Sort files to get a consistent order (optional)\n",
    "        worst_files.sort()\n",
    "        \n",
    "        # Show the top 3 worst predictions (or fewer if less are available)\n",
    "        num_to_show = min(3, len(worst_files))\n",
    "        \n",
    "        if num_to_show > 1:\n",
    "            fig, axes = plt.subplots(1, num_to_show, figsize=(15, 5))\n",
    "            for i in range(num_to_show):\n",
    "                axes[i].imshow(Image.open(worst_files[i]))\n",
    "                axes[i].set_title(f\"Worst #{i+1}\")\n",
    "                axes[i].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            # For a single image\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(Image.open(worst_files[0]))\n",
    "            plt.title(f\"Worst Prediction: {os.path.basename(worst_files[0])}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        logger.info(f\"Displayed {num_to_show} worst prediction visualizations from {worst_pred_dir}\")\n",
    "    else:\n",
    "        logger.warning(f\"No worst prediction visualizations found in {worst_pred_dir}.\")\n",
    "else:\n",
    "    logger.warning(f\"Worst predictions directory not found at {worst_pred_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Pipeline\n",
    "\n",
    "This notebook demonstrated the complete pipeline for training, evaluating, and predicting with the TCD-BARE model using the refactored codebase. All artifacts (configuration, checkpoints, metrics, visualizations) are saved in the directory specified by `config['output_dir']` (default: `./outputs`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
